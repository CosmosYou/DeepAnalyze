- data.train_data=['/root/data/gsm8k/train.parquet']
- data.val_data=['/root/data/gsm8k/validation.parquet']
- trainer.algorithm.advantage_estimator=grpo
- trainer.policy.model.path=Qwen/Qwen2.5-0.5B-Instruct
- trainer.placement.colocate_all=true
- trainer.policy.model.lora.rank=8
- trainer.policy.model.lora.alpha=8
- trainer.strategy=fsdp2
- trainer.placement.policy_num_gpus_per_node=1
- trainer.placement.ref_num_gpus_per_node=1
- generator.num_inference_engines=1
- generator.inference_engine_tensor_parallel_size=1
- trainer.epochs=2
- trainer.eval_batch_size=512
- trainer.eval_before_train=false
- trainer.eval_interval=5
- trainer.update_epochs_per_batch=1
- trainer.train_batch_size=512
- trainer.policy_mini_batch_size=256
- trainer.micro_forward_batch_size_per_gpu=32
- trainer.micro_train_batch_size_per_gpu=32
- trainer.ckpt_interval=10
- trainer.max_prompt_length=512
- generator.sampling_params.max_generate_length=512
- trainer.policy.optimizer_config.lr=3.0e-5
- trainer.algorithm.use_kl_loss=true
- generator.backend=vllm
- generator.run_engines_locally=true
- generator.weight_sync_backend=nccl
- generator.async_engine=true
- generator.batched=true
- environment.env_class=gsm8k
- generator.n_samples_per_prompt=5
- generator.gpu_memory_utilization=0.4
- trainer.logger=console
- trainer.project_name=gsm8k_0.5b_lora
- trainer.run_name=gsm8k_0.5b_lora_grpo
- trainer.resume_mode=null
- trainer.ckpt_path=/root/ckpts/gsm8k_0.5b_lora_ckpt
